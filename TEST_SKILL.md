# How to Test the Academic Content Generator Skill

Since the skill is now installed, you can test it by giving the tailored prompts below to the agent.

## Option 1: Functional Test (Content Generation)
Ask the agent to generate content for a specific topic. The skill should automatically pick up keywords like "chapter", "exercise", "explain", or "EdTech".

### Example Prompts:
1.  **"Write a micro-learning chapter on Supply and Demand for the Microeconomics course."**
    *   *Expected Output:* A structured Markdown file with a Micro-Concept, Interactive Task, and Feedback Logic, all in Hebrew.
2.  **"Create an interactive exercise for Quadratic Equations."**
    *   *Expected Output:* A math problem with LaTeX, anticipating common errors (e.g., sign errors).
3.  **"Explain the Accounting Equation to a beginner student."**
    *   *Expected Output:* A clear, jargon-free Hebrew explanation with a balancing task ($A=L+E$).

## Option 2: Structural Verification (Pedagogical Check)
Verify the agent follows the "Active Doer" philosophy (20% theory, 80% practice).

### Example Prompt:
*   **"Draft a lesson on Newton's Second Law. Ensure you follow the 'Active Doer' philosophy and include a specific 'Check, Correct, Learn' feedback loop."**

## Option 3: Manual "Writing-Skills" Test
If you want to debug the skill itself (as per the `writing-skills` protocol):
1.  Ask the agent to "simulate a student making a common mistake" on a topic.
2.  Then ask it to "generate the specific feedback" for that mistake.
3.  Verify the tone is "Authoritative yet Modern" (Gen Z style).

---
*Generated by Sikumnik Agent*
