#!/usr/bin/env python3
"""
Sikumnik Pipeline - Master orchestration script for PDF to JSON processing.

This script orchestrates the full pipeline flow:
  PDF -> Librarian (extract) -> Lecturer (transform) -> JSON

Usage Examples:
  # Process a single PDF file
  python pipeline.py input_materials/micro-economics/chapter-03/lecture.pdf

  # Process all chapters in a course
  python pipeline.py input_materials/micro-economics/ --batch

  # Dry run (show what would happen without executing)
  python pipeline.py input_materials/micro-economics/chapter-03/ --dry-run

  # Resume processing (skip already completed chapters)
  python pipeline.py input_materials/micro-economics/ --batch --resume

  # Verbose logging
  python pipeline.py input_materials/micro-economics/chapter-03/ -v

  # Force reprocessing (ignore existing outputs)
  python pipeline.py input_materials/micro-economics/chapter-03/ --force

Directory Structure:
  input_materials/
    micro-economics/
      chapter-XX/
        *.pdf
  courses/
    micro-economics/
      chapter-XX/
        librarian-output.md  (generated by librarian)
        chapter.json         (generated by lecturer)
"""

import os
import sys
import json
import shutil
import argparse
import logging
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Tuple, Dict
from dataclasses import dataclass, field
from enum import Enum


class PipelineStatus(Enum):
    """Status of a pipeline stage."""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class PipelineResult:
    """Result of processing a single chapter."""

    chapter_id: str
    pdf_path: Path
    librarian_output: Optional[Path] = None
    lecturer_output: Optional[Path] = None
    status: PipelineStatus = PipelineStatus.PENDING
    error_message: Optional[str] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    stages_completed: List[str] = field(default_factory=list)


class PipelineOrchestrator:
    """Orchestrates the full PDF to JSON pipeline."""

    def __init__(
        self,
        dry_run: bool = False,
        verbose: bool = False,
        resume: bool = False,
        force: bool = False,
        course_name: Optional[str] = None,
    ):
        self.dry_run = dry_run
        self.verbose = verbose
        self.resume = resume
        self.force = force
        self.course_name = course_name

        # Setup logging
        log_level = logging.DEBUG if verbose else logging.INFO
        logging.basicConfig(
            level=log_level,
            format="%(asctime)s [%(levelname)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        self.logger = logging.getLogger(__name__)

        # Scripts paths
        self.scripts_dir = Path(__file__).parent / "scripts"
        self.librarian_script = self.scripts_dir / "librarian_process.py"
        self.lecturer_script = self.scripts_dir / "lecturer_process.py"

        # Validate scripts exist
        if not self.dry_run:
            self._validate_scripts()

    def _validate_scripts(self):
        """Ensure required scripts exist."""
        if not self.librarian_script.exists():
            raise FileNotFoundError(
                f"Librarian script not found: {self.librarian_script}"
            )
        if not self.lecturer_script.exists():
            raise FileNotFoundError(
                f"Lecturer script not found: {self.lecturer_script}"
            )

    def _find_pdf_files(self, input_path: Path) -> List[Path]:
        """Find all PDF files in the input path."""
        pdfs = []

        if input_path.is_file():
            if input_path.suffix.lower() == ".pdf":
                pdfs.append(input_path)
        elif input_path.is_dir():
            # Look for PDFs in the directory and subdirectories
            for pdf_file in input_path.rglob("*.pdf"):
                pdfs.append(pdf_file)

        return sorted(pdfs)

    def _get_chapter_info(self, pdf_path: Path) -> Tuple[str, Path]:
        """Extract chapter ID and determine output directory from PDF path."""
        # Try to find chapter-XX pattern in the path
        path_str = str(pdf_path)

        # Extract chapter ID from path
        chapter_id = None
        for part in pdf_path.parts:
            if "chapter" in part.lower():
                chapter_id = part
                break

        if not chapter_id:
            # Fallback: use the parent directory name
            chapter_id = pdf_path.parent.name

        # Determine course name from path or use provided
        course = self.course_name
        if not course:
            for part in pdf_path.parts:
                if part in ["micro-economics", "accounting", "macro-economics"]:
                    course = part
                    break
            if not course:
                course = "unknown-course"

        # Construct output directory
        # input_materials/micro-economics/chapter-XX/ to courses/micro-economics/chapter-XX/
        if "input_materials" in path_str:
            output_dir = Path(path_str.replace("input_materials", "courses")).parent
        else:
            # Fallback: create parallel structure in courses/
            output_dir = Path("courses") / course / chapter_id

        return chapter_id, output_dir

    def _check_existing_outputs(self, output_dir: Path) -> Dict[str, bool]:
        """Check what outputs already exist."""
        librarian_output = output_dir / "librarian-output.md"
        lecturer_output = output_dir / "chapter.json"

        return {
            "librarian": librarian_output.exists()
            and librarian_output.stat().st_size > 0,
            "lecturer": lecturer_output.exists() and lecturer_output.stat().st_size > 0,
        }

    def _run_librarian(
        self, pdf_path: Path, output_dir: Path, topic: Optional[str] = None
    ) -> Tuple[bool, Optional[str]]:
        """Run the librarian process. Returns (success, error_message)."""
        output_path = output_dir / "librarian-output.md"

        cmd = [
            sys.executable,
            str(self.librarian_script),
            str(pdf_path),
            "--output",
            str(output_path),
        ]

        if topic:
            cmd.extend(["--topic", topic])

        self.logger.info(f"Running librarian: {' '.join(cmd)}")

        if self.dry_run:
            self.logger.info(f"[DRY RUN] Would execute: {' '.join(cmd)}")
            return True, None

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5 minute timeout
            )

            if result.returncode != 0:
                error_msg = (
                    f"Librarian failed with code {result.returncode}: {result.stderr}"
                )
                self.logger.error(error_msg)
                return False, error_msg

            if self.verbose and result.stdout:
                self.logger.debug(f"Librarian output:\n{result.stdout}")

            # Verify output was created
            if not output_path.exists():
                return False, "Librarian did not create output file"

            return True, None

        except subprocess.TimeoutExpired:
            return False, "Librarian timed out after 5 minutes"
        except Exception as e:
            return False, f"Librarian exception: {str(e)}"

    def _run_lecturer(
        self, markdown_path: Path, output_dir: Path
    ) -> Tuple[bool, Optional[str]]:
        """Run the lecturer process. Returns (success, error_message)."""
        output_path = output_dir / "chapter.json"

        cmd = [
            sys.executable,
            str(self.lecturer_script),
            str(markdown_path),
            "--output",
            str(output_path),
        ]

        self.logger.info(f"Running lecturer: {' '.join(cmd)}")

        if self.dry_run:
            self.logger.info(f"[DRY RUN] Would execute: {' '.join(cmd)}")
            return True, None

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,  # 5 minute timeout
            )

            if result.returncode != 0:
                error_msg = (
                    f"Lecturer failed with code {result.returncode}: {result.stderr}"
                )
                self.logger.error(error_msg)
                return False, error_msg

            if self.verbose and result.stdout:
                self.logger.debug(f"Lecturer output:\n{result.stdout}")

            # Verify and validate output
            if not output_path.exists():
                return False, "Lecturer did not create output file"

            # Validate JSON
            try:
                with open(output_path, "r", encoding="utf-8") as f:
                    json.load(f)
            except json.JSONDecodeError as e:
                return False, f"Invalid JSON output: {str(e)}"

            return True, None

        except subprocess.TimeoutExpired:
            return False, "Lecturer timed out after 5 minutes"
        except Exception as e:
            return False, f"Lecturer exception: {str(e)}"

    def _cleanup_on_failure(self, output_dir: Path):
        """Remove partially created files on failure."""
        if self.dry_run:
            return

        self.logger.info(f"Cleaning up partial outputs in {output_dir}")

        files_to_remove = [
            output_dir / "librarian-output.md",
            output_dir / "chapter.json",
        ]

        for file_path in files_to_remove:
            if file_path.exists():
                try:
                    file_path.unlink()
                    self.logger.debug(f"Removed: {file_path}")
                except Exception as e:
                    self.logger.warning(f"Could not remove {file_path}: {e}")

    def process_chapter(self, pdf_path: Path) -> PipelineResult:
        """Process a single chapter through the full pipeline."""
        chapter_id, output_dir = self._get_chapter_info(pdf_path)

        result = PipelineResult(
            chapter_id=chapter_id, pdf_path=pdf_path, start_time=datetime.now()
        )

        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"Processing: {chapter_id}")
        self.logger.info(f"PDF: {pdf_path}")
        self.logger.info(f"Output: {output_dir}")
        self.logger.info(f"{'=' * 60}")

        # Check for existing outputs
        existing = self._check_existing_outputs(output_dir)

        if self.resume and existing["lecturer"]:
            self.logger.info(f"Chapter {chapter_id} already complete. Skipping.")
            result.status = PipelineStatus.SKIPPED
            result.stages_completed = ["librarian", "lecturer"]
            result.librarian_output = output_dir / "librarian-output.md"
            result.lecturer_output = output_dir / "chapter.json"
            result.end_time = datetime.now()
            return result

        # Create output directory
        if not self.dry_run:
            output_dir.mkdir(parents=True, exist_ok=True)

        # Stage 1: Librarian
        librarian_output = output_dir / "librarian-output.md"

        if self.resume and existing["librarian"] and not self.force:
            self.logger.info(f"Librarian output exists. Skipping to lecturer.")
            result.librarian_output = librarian_output
            result.stages_completed.append("librarian")
        else:
            self.logger.info(f"Stage 1: Running librarian...")
            result.status = PipelineStatus.IN_PROGRESS

            success, error = self._run_librarian(
                pdf_path, output_dir, topic=chapter_id.replace("-", " ").title()
            )

            if not success:
                result.status = PipelineStatus.FAILED
                result.error_message = error
                result.end_time = datetime.now()
                self._cleanup_on_failure(output_dir)
                return result

            result.librarian_output = librarian_output
            result.stages_completed.append("librarian")
            self.logger.info(f"[OK] Librarian complete: {librarian_output}")

        # Stage 2: Lecturer
        self.logger.info(f"Stage 2: Running lecturer...")

        success, error = self._run_lecturer(result.librarian_output, output_dir)

        if not success:
            result.status = PipelineStatus.FAILED
            result.error_message = error
            result.end_time = datetime.now()
            self._cleanup_on_failure(output_dir)
            return result

        result.lecturer_output = output_dir / "chapter.json"
        result.stages_completed.append("lecturer")
        result.status = PipelineStatus.COMPLETED
        result.end_time = datetime.now()

        self.logger.info(f"[OK] Lecturer complete: {result.lecturer_output}")

        if result.end_time and result.start_time:
            duration = (result.end_time - result.start_time).total_seconds()
            self.logger.info(f"Pipeline complete in {duration:.1f}s")
        else:
            self.logger.info("Pipeline complete")

        return result

    def process_batch(self, input_path: Path) -> List[PipelineResult]:
        """Process multiple chapters in batch mode."""
        pdf_files = self._find_pdf_files(input_path)

        if not pdf_files:
            self.logger.error(f"No PDF files found in {input_path}")
            return []

        self.logger.info(f"\nFound {len(pdf_files)} PDF file(s) to process")

        results = []
        completed = 0
        failed = 0
        skipped = 0

        for i, pdf_file in enumerate(pdf_files, 1):
            self.logger.info(f"\n[{i}/{len(pdf_files)}] Processing {pdf_file.name}")

            result = self.process_chapter(pdf_file)
            results.append(result)

            if result.status == PipelineStatus.COMPLETED:
                completed += 1
            elif result.status == PipelineStatus.FAILED:
                failed += 1
                self.logger.error(f"âœ— Failed: {result.error_message}")
            elif result.status == PipelineStatus.SKIPPED:
                skipped += 1

        # Summary
        self.logger.info(f"\n{'=' * 60}")
        self.logger.info("BATCH SUMMARY")
        self.logger.info(f"{'=' * 60}")
        self.logger.info(f"Total:     {len(results)}")
        self.logger.info(f"Completed: {completed}")
        self.logger.info(f"Failed:    {failed}")
        self.logger.info(f"Skipped:   {skipped}")

        return results

    def generate_report(
        self, results: List[PipelineResult], output_path: Optional[Path] = None
    ):
        """Generate a JSON report of pipeline results."""
        report = {
            "timestamp": datetime.now().isoformat(),
            "total": len(results),
            "completed": sum(
                1 for r in results if r.status == PipelineStatus.COMPLETED
            ),
            "failed": sum(1 for r in results if r.status == PipelineStatus.FAILED),
            "skipped": sum(1 for r in results if r.status == PipelineStatus.SKIPPED),
            "chapters": [],
        }

        for result in results:
            chapter_report = {
                "chapter_id": result.chapter_id,
                "pdf_path": str(result.pdf_path),
                "status": result.status.value,
                "stages_completed": result.stages_completed,
                "librarian_output": str(result.librarian_output)
                if result.librarian_output
                else None,
                "lecturer_output": str(result.lecturer_output)
                if result.lecturer_output
                else None,
                "error": result.error_message,
                "duration_seconds": (
                    (result.end_time - result.start_time).total_seconds()
                    if result.end_time and result.start_time
                    else None
                ),
            }
            report["chapters"].append(chapter_report)

        if output_path:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Report saved to: {output_path}")

        return report


def main():
    parser = argparse.ArgumentParser(
        description="Sikumnik Pipeline: Orchestrate PDF to JSON processing",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process single PDF
  python pipeline.py input_materials/micro-economics/chapter-03/lecture.pdf

  # Batch process all chapters in a course
  python pipeline.py input_materials/micro-economics/ --batch

  # Dry run to preview what would happen
  python pipeline.py input_materials/micro-economics/ --batch --dry-run

  # Resume from previous run (skip completed chapters)
  python pipeline.py input_materials/micro-economics/ --batch --resume

  # Force reprocessing (ignore existing outputs)
  python pipeline.py input_materials/micro-economics/chapter-03/ --force

  # Generate a report file
  python pipeline.py input_materials/micro-economics/ --batch --report pipeline-report.json
        """,
    )

    parser.add_argument(
        "input", type=Path, help="Input PDF file or directory containing PDFs"
    )

    parser.add_argument(
        "--batch", action="store_true", help="Process all PDFs in the input directory"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without executing",
    )

    parser.add_argument(
        "--resume",
        action="store_true",
        help="Skip chapters that already have complete outputs",
    )

    parser.add_argument(
        "--force", action="store_true", help="Force reprocessing even if outputs exist"
    )

    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )

    parser.add_argument(
        "--course", type=str, help="Course name (auto-detected if not specified)"
    )

    parser.add_argument("--report", type=Path, help="Path to save JSON report")

    args = parser.parse_args()

    # Validate input exists
    if not args.input.exists():
        print(f"Error: Input path does not exist: {args.input}", file=sys.stderr)
        sys.exit(1)

    # If not batch mode and input is a directory, error
    if not args.batch and args.input.is_dir():
        print(
            f"Error: Input is a directory. Use --batch to process all PDFs, "
            f"or specify a single PDF file.",
            file=sys.stderr,
        )
        sys.exit(1)

    # Initialize orchestrator
    orchestrator = PipelineOrchestrator(
        dry_run=args.dry_run,
        verbose=args.verbose,
        resume=args.resume,
        force=args.force,
        course_name=args.course,
    )

    # Process
    if args.batch:
        results = orchestrator.process_batch(args.input)
    else:
        result = orchestrator.process_chapter(args.input)
        results = [result]

    # Generate report if requested
    if args.report:
        orchestrator.generate_report(results, args.report)

    # Exit with error code if any failures
    failed_count = sum(1 for r in results if r.status == PipelineStatus.FAILED)
    if failed_count > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()
